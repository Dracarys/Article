# 由离屏渲染引申开去

一直对离屏渲染由疑惑，始终没能解开，归根结底还是自己基础薄弱，对计算机图形理解的不够。比较巧的是，前天即刻团队突然在网上分享了一篇[《关于iOS离屏渲染的深入研究》](https://zhuanlan.zhihu.com/p/72653360)，一时间获得了不错的转发，果断第一时间拜读，获益颇多，不得不说他们确实花费了不少功夫。但遗憾的是作者对于离屏渲染发生时机，以及为什么不得不“离屏”的原因没能深入阐述，给出一个明确的答案，只是笼统的解释为“不能在单次遍历中完成渲染”。这不但没能打消自己先前的疑虑，反而更加困惑了。所以决定自己动手探究一番。

## 答案
离屏渲染发生在渲染流程最后的“融合”阶段。假设有10层像素需要融合，默认是从低向上逐层进行的，但是因为“透明”、“遮罩”等效果，不得不需要先融合后面的，然后再与前面的结果进行融合，而这个不得不就是导致离屏渲染的直接原因。因为既要保存先前的融合结果，又要另辟一块地方对后面的像素进行融合，这就导致了所谓的“离屏”，因为融合过程是发生在帧缓存中的，这块另辟的空间显然离开了这里。

如果看过答案还不是很明白，那么请继续。当然我也不保证通过这篇文章就一定能阐述清楚，毕竟个人知识水平有限，不过即使在有限，那怕能起到那么一点点抛砖引玉的作用，也总是好的。

## 概念
在继续之前，我们先来统一两个概念，可能与之前的理解有不同。

### 什么是离屏渲染？
这里的离屏渲染是指离开当前屏幕帧缓存发生的由硬件加速的渲染处理（严格来说应该是后帧缓存，具体说明请参见 OpenGL 渲染流程一节）。

那么通过 Core Graphics 框架发生的渲染是不是离屏渲染呢？广义上是的，因为它也不是发生在当前屏幕的帧缓存，而是 CPU 单独开辟一块空间进行的。为了与更擅长渲染工作的 GPU 进行区分，这种渲染方式被称为“软渲染”，与“软解码”概念类似。

### Pipeline
很多地方把它翻译成“管线”。在这里把它译为“流程”或“流水线”，指一系列的承前启后的处理过程。OpenGL 的渲染过程正是如此。

## OpenGL 渲染流程
OpenGL 是一种应用程序编程接口，它是一个可以对图形硬件设备特性进行访问的软件库，可以用于设置所需的对象、图像和操作，以便开发交互式的3维计算机图形应用程序。

要想通过 OpenGL 渲染一个图像，需要经过一系列的操作，而这一些列的操作就被称为 Pipline：

- 生成、绑定顶点数据：这些数据决定了最终图像显示的位置、颜色。并告诉 GPU 如何读取和解析这些数据；
- 顶点着色：OpenGL通过它来对顶点数据进行处理。
- 细分着色
- 几何着色
- 图元装配
- 剪切：有些顶点可能会落在适视框（viewport）之外，也就是屏幕可视范围之外，显然范围外的数据无需处理，会被剪切掉，如此做也是为了提高效率。
- 光栅化：裁切过后的图元会进行光栅化，生成片元（Gragment），可以把它看作待选像素，也就是可以放置到帧缓存中的像素，但是否会最终被现实到屏幕上，还有待进一步的操作。
- 片元着色：通过片元着色器（一段计算颜色的逻辑程序）来计算片元最终的颜色（但不一定是最终在屏幕上显示的颜色）；


### 在这个阶段会使用深度测试（depth test，或者通常也称作 z-buffering）和模板测试（stencil test）的方式来决定一个片元是否可见。

### 融合（blending）


## 诱因分析
## 透明
双层透明

### 遮罩


上面的理论假设已经完毕，下面我们来看看如何证明这一假设：

## 证明

### UIKit pipline

### Core Aniamtin guide

### 双透明的例子

### 圆角裁切的例子

### 离屏渲染标黄的截图

### 疑问

- 为什么离屏会有这么高的代价，因为阻塞了主融合“线程”（因为对 GPU 的理解几乎空白，所以这里用线程可能不准确），需要等待子任务的完成吗？


### 参考

- [《关于iOS离屏渲染的深入研究》](https://zhuanlan.zhihu.com/p/72653360)
- 《OpenGL 编程指南（原书第8版）》作者：Dave Shreiner、Graham Sellers、John Kessenich、Bill Licean-Kane，译者：王锐

